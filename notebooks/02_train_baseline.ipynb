{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f16a4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTrain baseline ResNet-18 on PTB-XL dataset.\\n\\nGoal: Get a working model quickly that can:\\n1. Classify ECG signals into diagnostic categories\\n2. Map diagnoses to clinical actions\\n3. Be deployed in the FastAPI service\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FILE: notebooks/02_train_baseline.ipynb\n",
    "# ============================================================\n",
    "# This is a Python script version - convert to notebook with:\n",
    "# jupytext --to notebook 02_train_baseline.py\n",
    "\n",
    "\"\"\"\n",
    "Train baseline ResNet-18 on PTB-XL dataset.\n",
    "\n",
    "Goal: Get a working model quickly that can:\n",
    "1. Classify ECG signals into diagnostic categories\n",
    "2. Map diagnoses to clinical actions\n",
    "3. Be deployed in the FastAPI service\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f869f51",
   "metadata": {},
   "source": [
    "# Baseline ECG Clinical Action Recommender\n",
    "\n",
    "Train ResNet-18 on PTB-XL for multi-label classification of diagnostic superclasses.\n",
    "Then map diagnoses to clinical actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0f12e4",
   "metadata": {
    "title": "Imports"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wfdb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import our model\n",
    "from src.ml.models.resnet1d import resnet18_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066dbe",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2dd10",
   "metadata": {
    "title": "Configuration"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\")\n",
    "RECORDS_DIR = DATA_DIR / \"records500\"  # Use 500 Hz\n",
    "DATABASE_FILE = DATA_DIR / \"ptbxl_database.csv\"\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fb968",
   "metadata": {
    "title": "Load metadata"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATABASE_FILE)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1c5b9",
   "metadata": {
    "title": "Parse SCP codes"
   },
   "outputs": [],
   "source": [
    "# PTB-XL stores diagnostic codes as string dictionaries\n",
    "df['scp_codes_dict'] = df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Extract diagnostic superclass (main diagnostic category)\n",
    "def get_superclass_labels(scp_dict, superclass_mapping):\n",
    "    \"\"\"Extract superclass labels from SCP codes.\"\"\"\n",
    "    labels = set()\n",
    "    for code in scp_dict.keys():\n",
    "        if code in superclass_mapping:\n",
    "            labels.add(superclass_mapping[code])\n",
    "    return labels\n",
    "\n",
    "# Load superclass mapping (from PTB-XL paper)\n",
    "# NORM: Normal ECG\n",
    "# MI: Myocardial Infarction\n",
    "# STTC: ST/T Change\n",
    "# CD: Conduction Disturbance\n",
    "# HYP: Hypertrophy\n",
    "\n",
    "# Simplified mapping of common codes to superclasses\n",
    "SUPERCLASS_MAP = {\n",
    "    'NORM': 'NORM',\n",
    "    'IMI': 'MI', 'AMI': 'MI', 'LMI': 'MI', 'ALMI': 'MI', 'ILMI': 'MI',\n",
    "    'ASMI': 'MI', 'PMI': 'MI', 'INJAL': 'MI', 'IPLMI': 'MI', 'IPMI': 'MI',\n",
    "    'NST_': 'STTC', 'DIG': 'STTC', 'LNGQT': 'STTC', 'ISC_': 'STTC',\n",
    "    'STTC': 'STTC', 'STD_': 'STTC', 'STE_': 'STTC',\n",
    "    'IRBBB': 'CD', 'CRBBB': 'CD', 'CLBBB': 'CD', 'ILBBB': 'CD',\n",
    "    'LAFB': 'CD', 'LPFB': 'CD', 'IVCD': 'CD', 'WPW': 'CD',\n",
    "    'LVH': 'HYP', 'RVH': 'HYP', 'LAO/LAE': 'HYP', 'RAO/RAE': 'HYP',\n",
    "}\n",
    "\n",
    "df['superclass_labels'] = df.scp_codes_dict.apply(\n",
    "    lambda x: get_superclass_labels(x, SUPERCLASS_MAP)\n",
    ")\n",
    "\n",
    "# Filter to records with at least one superclass label\n",
    "df = df[df.superclass_labels.apply(len) > 0].reset_index(drop=True)\n",
    "print(f\"\\nRecords with superclass labels: {len(df)}\")\n",
    "\n",
    "# Create multi-label encoding\n",
    "SUPERCLASSES = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
    "def encode_labels(label_set):\n",
    "    return [1 if sc in label_set else 0 for sc in SUPERCLASSES]\n",
    "\n",
    "df['labels'] = df.superclass_labels.apply(encode_labels)\n",
    "\n",
    "# Check label distribution\n",
    "label_counts = df.labels.apply(lambda x: sum(x)).value_counts().sort_index()\n",
    "print(\"\\nLabel distribution (# of labels per ECG):\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nClass frequencies:\")\n",
    "for i, sc in enumerate(SUPERCLASSES):\n",
    "    count = sum(df.labels.apply(lambda x: x[i]))\n",
    "    print(f\"  {sc}: {count} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aaadf7",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Train/val/test split"
   },
   "outputs": [],
   "source": [
    "# Use stratified split on the most common label\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df.labels.apply(lambda x: x[0])\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, random_state=42, stratify=train_df.labels.apply(lambda x: x[0])\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(train_df)}\")\n",
    "print(f\"  Val:   {len(val_df)}\")\n",
    "print(f\"  Test:  {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2170e",
   "metadata": {},
   "source": [
    "## 2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f1a18",
   "metadata": {
    "title": "ECG Dataset"
   },
   "outputs": [],
   "source": [
    "class PTBXLDataset(Dataset):\n",
    "    \"\"\"PTB-XL dataset for ECG signals.\"\"\"\n",
    "\n",
    "    def __init__(self, df, records_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.records_dir = Path(records_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load ECG signal\n",
    "        record_path = self.records_dir / row.filename_hr  # Use high-res (500Hz)\n",
    "        record = wfdb.rdsamp(str(record_path.with_suffix('')))\n",
    "        signal = record[0]  # (5000, 12) numpy array\n",
    "\n",
    "        # Transpose to (12, 5000) for Conv1D\n",
    "        signal = signal.T.astype(np.float32)\n",
    "\n",
    "        # Normalize (simple z-score per lead)\n",
    "        signal = (signal - signal.mean(axis=1, keepdims=True)) / (\n",
    "            signal.std(axis=1, keepdims=True) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Get labels\n",
    "        labels = np.array(row.labels, dtype=np.float32)\n",
    "\n",
    "        # Patient context (for future use)\n",
    "        age = row.age / 100.0  # Normalize to [0, 1] roughly\n",
    "        sex = 1.0 if row.sex == 1 else 0.0  # Male=1, Female=0\n",
    "        patient_context = np.array([age, sex, 0.0], dtype=np.float32)  # 3rd feature placeholder\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return {\n",
    "            'signal': torch.from_numpy(signal),\n",
    "            'labels': torch.from_numpy(labels),\n",
    "            'patient_context': torch.from_numpy(patient_context),\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PTBXLDataset(train_df, RECORDS_DIR)\n",
    "val_dataset = PTBXLDataset(val_df, RECORDS_DIR)\n",
    "test_dataset = PTBXLDataset(test_df, RECORDS_DIR)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Test loading one batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nSample batch shapes:\")\n",
    "print(f\"  Signal: {sample_batch['signal'].shape}\")\n",
    "print(f\"  Labels: {sample_batch['labels'].shape}\")\n",
    "print(f\"  Context: {sample_batch['patient_context'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bffd3b",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6957646",
   "metadata": {
    "title": "Initialize model"
   },
   "outputs": [],
   "source": [
    "model = resnet18_1d(num_classes=len(SUPERCLASSES), include_patient_context=False)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"\\nModel: ResNet-18 1D\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09d007",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Loss function and optimizer"
   },
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb95c",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Training loop"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        signals = batch['signal'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            signals = batch['signal'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    # AUC-ROC per class\n",
    "    aucs = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        if len(np.unique(all_labels[:, i])) > 1:  # Only if both classes present\n",
    "            auc = roc_auc_score(all_labels[:, i], all_preds[:, i])\n",
    "            aucs.append(auc)\n",
    "\n",
    "    macro_auc = np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    return avg_loss, macro_auc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9283c6",
   "metadata": {
    "title": "Start MLflow run"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"ecg-baseline\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"resnet18-baseline\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model\": \"resnet18_1d\",\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"train_size\": len(train_df),\n",
    "        \"val_size\": len(val_df),\n",
    "        \"test_size\": len(test_df),\n",
    "    })\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_auc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val AUC:    {val_auc:.4f}\")\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_auc\": val_auc,\n",
    "        }, step=epoch)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save(model.state_dict(), \"../models/best_model.pth\")\n",
    "            mlflow.log_metric(\"best_val_auc\", best_val_auc)\n",
    "            print(f\"  ✓ Best model saved (AUC: {best_val_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0412f68",
   "metadata": {
    "title": "Final evaluation on test set"
   },
   "outputs": [],
   "source": [
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"../models/best_model.pth\"))\n",
    "\n",
    "    test_loss, test_auc, test_preds, test_labels = evaluate(\n",
    "        model, test_loader, criterion, DEVICE\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "\n",
    "    # Per-class AUC\n",
    "    print(\"\\nPer-class AUC:\")\n",
    "    for i, sc in enumerate(SUPERCLASSES):\n",
    "        if len(np.unique(test_labels[:, i])) > 1:\n",
    "            auc = roc_auc_score(test_labels[:, i], test_preds[:, i])\n",
    "            print(f\"  {sc}: {auc:.4f}\")\n",
    "\n",
    "    # Log test metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_auc\": test_auc,\n",
    "    })\n",
    "\n",
    "    # Log model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    print(\"\\n✓ Model logged to MLflow\")\n",
    "    print(f\"✓ Run ID: {mlflow.active_run().info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acaee3d",
   "metadata": {},
   "source": [
    "## 4. Diagnostic → Action Mapping\n",
    "\n",
    "Define mapping from diagnostic superclasses to clinical actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7886d",
   "metadata": {
    "title": "Action mapping"
   },
   "outputs": [],
   "source": [
    "DIAGNOSTIC_TO_ACTIONS = {\n",
    "    'MI': [\n",
    "        {\n",
    "            'action': 'Activate cath lab',\n",
    "            'urgency': 'immediate',\n",
    "            'reasoning': 'Myocardial infarction detected - requires immediate intervention'\n",
    "        },\n",
    "        {\n",
    "            'action': 'Administer aspirin 325mg',\n",
    "            'urgency': 'immediate',\n",
    "            'reasoning': 'Antiplatelet therapy for acute MI'\n",
    "        },\n",
    "        {\n",
    "            'action': 'Order troponin stat',\n",
    "            'urgency': 'immediate',\n",
    "            'reasoning': 'Confirm cardiac injury with biomarkers'\n",
    "        },\n",
    "    ],\n",
    "    'STTC': [\n",
    "        {\n",
    "            'action': 'Order troponin levels',\n",
    "            'urgency': 'urgent',\n",
    "            'reasoning': 'ST/T changes may indicate ischemia'\n",
    "        },\n",
    "        {\n",
    "            'action': '12-lead ECG in 6 hours',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'Monitor for dynamic changes'\n",
    "        },\n",
    "        {\n",
    "            'action': 'Cardiology consult',\n",
    "            'urgency': 'urgent',\n",
    "            'reasoning': 'Specialist review of ST/T abnormalities'\n",
    "        },\n",
    "    ],\n",
    "    'CD': [\n",
    "        {\n",
    "            'action': 'Review medications',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'Conduction disturbance - check for QT-prolonging drugs'\n",
    "        },\n",
    "        {\n",
    "            'action': 'Consider 24-hour Holter monitor',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'Assess for intermittent conduction blocks'\n",
    "        },\n",
    "    ],\n",
    "    'HYP': [\n",
    "        {\n",
    "            'action': 'Order echocardiogram',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'Hypertrophy detected - assess cardiac function'\n",
    "        },\n",
    "        {\n",
    "            'action': 'Blood pressure monitoring',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'Evaluate for hypertension as underlying cause'\n",
    "        },\n",
    "    ],\n",
    "    'NORM': [\n",
    "        {\n",
    "            'action': 'Normal ECG - discharge with instructions',\n",
    "            'urgency': 'routine',\n",
    "            'reasoning': 'No acute abnormalities detected'\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"\\nDiagnostic → Action Mapping:\")\n",
    "for dx, actions in DIAGNOSTIC_TO_ACTIONS.items():\n",
    "    print(f\"\\n{dx}:\")\n",
    "    for action in actions:\n",
    "        print(f\"  • {action['action']} ({action['urgency']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3d3d7",
   "metadata": {
    "title": "Save mapping as JSON for API"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "mapping_file = Path(\"../src/ml/inference/action_mapping.json\")\n",
    "mapping_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(mapping_file, 'w') as f:\n",
    "    json.dump(DIAGNOSTIC_TO_ACTIONS, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Action mapping saved to {mapping_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Check MLflow UI (http://localhost:5000) for experiment tracking\")\n",
    "print(\"2. Model saved to: ../models/best_model.pth\")\n",
    "print(\"3. Ready to integrate into FastAPI service\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ecgEnv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
